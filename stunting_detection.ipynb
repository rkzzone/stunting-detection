{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acc6cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.52.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.16.3)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: plotly in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.5.0)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<7,>=4.0 (from streamlit)\n",
      "  Downloading altair-6.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\muham\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (12.0.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.33.0)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\muham\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muham\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: narwhals>=1.27.1 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muham\\appdata\\roaming\\python\\python313\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\muham\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.28.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muham\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading streamlit-1.52.1-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/9.0 MB 2.1 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/9.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.6/9.0 MB 2.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.1/9.0 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.4/9.0 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.4/9.0 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.6/9.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.1/9.0 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.4/9.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.9/9.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.5/9.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.2/9.0 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.0/9.0 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 7.3/9.0 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.7/9.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 2.7 MB/s  0:00:03\n",
      "Downloading altair-6.0.0-py3-none-any.whl (795 kB)\n",
      "   ---------------------------------------- 0.0/795.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/795.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/795.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/795.4 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/795.4 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 524.3/795.4 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 524.3/795.4 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 524.3/795.4 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 795.4/795.4 kB 600.8 kB/s  0:00:01\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.9 MB 1.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.8/6.9 MB 1.4 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.3/6.9 MB 1.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.6/6.9 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.8/6.9 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/6.9 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.6/6.9 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.9/6.9 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.9/6.9 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.9/6.9 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.7/6.9 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.5/6.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/6.9 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 2.3 MB/s  0:00:03\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/28.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/28.0 MB 2.0 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 1.0/28.0 MB 2.0 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.6/28.0 MB 2.1 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 2.4/28.0 MB 2.4 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.1/28.0 MB 2.7 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 3.7/28.0 MB 2.8 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 4.7/28.0 MB 3.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 5.8/28.0 MB 3.3 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 6.8/28.0 MB 3.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.9/28.0 MB 3.7 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 9.2/28.0 MB 3.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 10.0/28.0 MB 3.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 11.0/28.0 MB 4.0 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.8/28.0 MB 3.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 12.3/28.0 MB 3.9 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 13.4/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 13.9/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 14.7/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 15.5/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 16.3/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 17.3/28.0 MB 3.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 18.4/28.0 MB 4.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 19.1/28.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 19.9/28.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 20.7/28.0 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 21.5/28.0 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 22.0/28.0 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 22.5/28.0 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 23.1/28.0 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 23.6/28.0 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 25.2/28.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.0/28.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.5/28.0 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.3/28.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 3.8 MB/s  0:00:07\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: watchdog, toml, tenacity, smmap, pyarrow, et-xmlfile, click, cachetools, blinker, pydeck, openpyxl, gitdb, gitpython, altair, streamlit\n",
      "\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------------- -----------------------  6/15 [click]\n",
      "   ------------------------ ---------------  9/15 [pydeck]\n",
      "   -------------------------- ------------- 10/15 [openpyxl]\n",
      "   -------------------------- ------------- 10/15 [openpyxl]\n",
      "   -------------------------------- ------- 12/15 [gitpython]\n",
      "   ---------------------------------- ----- 13/15 [altair]\n",
      "   ---------------------------------- ----- 13/15 [altair]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ---------------------------------------- 15/15 [streamlit]\n",
      "\n",
      "Successfully installed altair-6.0.0 blinker-1.9.0 cachetools-6.2.2 click-8.3.1 et-xmlfile-2.0.0 gitdb-4.0.12 gitpython-3.1.45 openpyxl-3.1.5 pyarrow-22.0.0 pydeck-0.9.1 smmap-5.0.2 streamlit-1.52.1 tenacity-9.1.2 toml-0.10.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script watchmedo.exe is installed in 'c:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script streamlit.exe is installed in 'c:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit pandas numpy scikit-learn scipy openpyxl plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194e1e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully\n",
      "ğŸ“… Timestamp: 2025-12-08 21:43:05.603679\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")\n",
    "print(f\"ğŸ“… Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d77aa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Base Path: C:\\Users\\muham\\Project\\Stunting\n",
      "ğŸ“‚ Data Path: C:\\Users\\muham\\Project\\Stunting\\data_balita.csv\n",
      "ğŸ“‚ Model Directory: C:\\Users\\muham\\Project\\Stunting\\models\n",
      "âœ… Dataset found!\n"
     ]
    }
   ],
   "source": [
    "# Path configuration\n",
    "BASE_PATH = r\"C:\\Users\\muham\\Project\\Stunting\"\n",
    "DATA_PATH = os.path.join(BASE_PATH, \"data_balita.csv\")\n",
    "MODEL_DIR = os.path.join(BASE_PATH, \"models\")\n",
    "\n",
    "# Create directories if not exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_PATH, \"utils\"), exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“‚ Base Path: {BASE_PATH}\")\n",
    "print(f\"ğŸ“‚ Data Path: {DATA_PATH}\")\n",
    "print(f\"ğŸ“‚ Model Directory: {MODEL_DIR}\")\n",
    "\n",
    "# Check if data file exists\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(\"âœ… Dataset found!\")\n",
    "else:\n",
    "    print(\"âŒ Dataset not found! Please ensure data_balita.csv is in the correct location.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472ea15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Dataset Shape: 120999 rows Ã— 4 columns\n",
      "\n",
      "ğŸ“‹ Columns: ['Umur (bulan)', 'Jenis Kelamin', 'Tinggi Badan (cm)', 'Status Gizi']\n",
      "\n",
      "ğŸ“Š First 5 rows:\n",
      "   Umur (bulan) Jenis Kelamin  Tinggi Badan (cm)       Status Gizi\n",
      "0             0     laki-laki          44.591973           stunted\n",
      "1             0     laki-laki          56.705203            tinggi\n",
      "2             0     laki-laki          46.863358            normal\n",
      "3             0     laki-laki          47.508026            normal\n",
      "4             0     laki-laki          42.743494  severely stunted\n",
      "\n",
      "ğŸ“ˆ Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120999 entries, 0 to 120998\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Umur (bulan)       120999 non-null  int64  \n",
      " 1   Jenis Kelamin      120999 non-null  object \n",
      " 2   Tinggi Badan (cm)  120999 non-null  float64\n",
      " 3   Status Gizi        120999 non-null  object \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 3.7+ MB\n",
      "\n",
      "ğŸ“Š Statistical Summary:\n",
      "        Umur (bulan)  Tinggi Badan (cm)\n",
      "count  120999.000000      120999.000000\n",
      "mean       30.173803          88.655434\n",
      "std        17.575119          17.300997\n",
      "min         0.000000          40.010437\n",
      "25%        15.000000          77.000000\n",
      "50%        30.000000          89.800000\n",
      "75%        45.000000         101.200000\n",
      "max        60.000000         128.000000\n",
      "\n",
      "ğŸ¯ Status Gizi Distribution:\n",
      "Status Gizi\n",
      "normal              67755\n",
      "severely stunted    19869\n",
      "tinggi              19560\n",
      "stunted             13815\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage:\n",
      "Status Gizi\n",
      "normal              55.996331\n",
      "severely stunted    16.420797\n",
      "tinggi              16.165423\n",
      "stunted             11.417450\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "ğŸ‘¥ Gender Distribution:\n",
      "Jenis Kelamin\n",
      "perempuan    61002\n",
      "laki-laki    59997\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âš ï¸ Missing Values:\n",
      "Umur (bulan)         0\n",
      "Jenis Kelamin        0\n",
      "Tinggi Badan (cm)    0\n",
      "Status Gizi          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nğŸ“ Dataset Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"\\nğŸ“‹ Columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nğŸ“Š First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nğŸ“ˆ Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nğŸ“Š Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nğŸ¯ Status Gizi Distribution:\")\n",
    "print(df['Status Gizi'].value_counts())\n",
    "print(\"\\nPercentage:\")\n",
    "print(df['Status Gizi'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nğŸ‘¥ Gender Distribution:\")\n",
    "print(df['Jenis Kelamin'].value_counts())\n",
    "\n",
    "print(\"\\nâš ï¸ Missing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a88142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”„ DATA PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "âœ… Removed 81574 duplicate rows\n",
      "âœ… Removed rows with missing values\n",
      "ğŸ“ Clean dataset shape: (39425, 4)\n",
      "\n",
      "ğŸ”„ Encoding categorical features...\n",
      "\n",
      "âœ… Encoding completed:\n",
      "Gender mapping:\n",
      "  - laki-laki: 0\n",
      "  - perempuan: 1\n",
      "\n",
      "Status Gizi mapping:\n",
      "  - normal: 0\n",
      "  - severely stunted: 1\n",
      "  - stunted: 2\n",
      "  - tinggi: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ğŸ”„ DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Remove duplicates\n",
    "df_clean = df.drop_duplicates()\n",
    "print(f\"\\nâœ… Removed {df.shape[0] - df_clean.shape[0]} duplicate rows\")\n",
    "\n",
    "# Remove missing values\n",
    "df_clean = df_clean.dropna()\n",
    "print(f\"âœ… Removed rows with missing values\")\n",
    "print(f\"ğŸ“ Clean dataset shape: {df_clean.shape}\")\n",
    "\n",
    "# Encode categorical features\n",
    "print(\"\\nğŸ”„ Encoding categorical features...\")\n",
    "\n",
    "# Gender encoding\n",
    "gender_encoder = LabelEncoder()\n",
    "df_clean['Jenis Kelamin Encoded'] = gender_encoder.fit_transform(df_clean['Jenis Kelamin'])\n",
    "\n",
    "# Status Gizi encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df_clean['Status Gizi Encoded'] = label_encoder.fit_transform(df_clean['Status Gizi'])\n",
    "\n",
    "print(\"\\nâœ… Encoding completed:\")\n",
    "print(\"Gender mapping:\")\n",
    "for cls, encoded in zip(gender_encoder.classes_, gender_encoder.transform(gender_encoder.classes_)):\n",
    "    print(f\"  - {cls}: {encoded}\")\n",
    "\n",
    "print(\"\\nStatus Gizi mapping:\")\n",
    "for cls, encoded in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):\n",
    "    print(f\"  - {cls}: {encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bc25d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¯ FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Features shape: (39425, 3)\n",
      "ğŸ¯ Target shape: (39425,)\n",
      "\n",
      "ğŸ“‹ Feature names: ['Umur (bulan)', 'Jenis Kelamin Encoded', 'Tinggi Badan (cm)']\n",
      "ğŸ¯ Target classes: ['normal', 'severely stunted', 'stunted', 'tinggi']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¯ FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "feature_columns = ['Umur (bulan)', 'Jenis Kelamin Encoded', 'Tinggi Badan (cm)']\n",
    "X = df_clean[feature_columns].values\n",
    "y = df_clean['Status Gizi Encoded'].values\n",
    "\n",
    "print(f\"\\nğŸ“Š Features shape: {X.shape}\")\n",
    "print(f\"ğŸ¯ Target shape: {y.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Feature names: {feature_columns}\")\n",
    "print(f\"ğŸ¯ Target classes: {label_encoder.classes_.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb1286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ‚ï¸ TRAIN-TEST SPLIT\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Training set: 31540 samples (80.0%)\n",
      "ğŸ“Š Test set: 7885 samples (20.0%)\n",
      "\n",
      "ğŸ¯ Training set class distribution:\n",
      "  - normal: 17211 (54.6%)\n",
      "  - severely stunted: 5216 (16.5%)\n",
      "  - stunted: 3534 (11.2%)\n",
      "  - tinggi: 5579 (17.7%)\n",
      "\n",
      "ğŸ¯ Test set class distribution:\n",
      "  - normal: 4303 (54.6%)\n",
      "  - severely stunted: 1304 (16.5%)\n",
      "  - stunted: 883 (11.2%)\n",
      "  - tinggi: 1395 (17.7%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ‚ï¸ TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split data dengan stratify untuk menjaga proporsi kelas\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"ğŸ“Š Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nğŸ¯ Training set class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for cls, count in zip(label_encoder.inverse_transform(unique), counts):\n",
    "    print(f\"  - {cls}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nğŸ¯ Test set class distribution:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for cls, count in zip(label_encoder.inverse_transform(unique), counts):\n",
    "    print(f\"  - {cls}: {count} ({count/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8207639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âš–ï¸ FEATURE SCALING\n",
      "================================================================================\n",
      "\n",
      "âœ… Features scaled using StandardScaler\n",
      "\n",
      "ğŸ“Š Training set mean: [-7.38233190e-17  1.37977419e-15 -1.55950219e-14]\n",
      "ğŸ“Š Training set std: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âš–ï¸ FEATURE SCALING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nâœ… Features scaled using StandardScaler\")\n",
    "print(f\"\\nğŸ“Š Training set mean: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"ğŸ“Š Training set std: {X_train_scaled.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a152533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Parameter grid:\n",
      "  - n_neighbors: [3, 5, 7, 9, 11, 13, 15]\n",
      "  - weights: ['uniform', 'distance']\n",
      "  - metric: ['euclidean', 'manhattan', 'minkowski']\n",
      "\n",
      "ğŸ”„ Performing Grid Search with 5-fold Cross-Validation...\n",
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n",
      "\n",
      "âœ… Grid Search completed!\n",
      "\n",
      "ğŸ† Best parameters: {'metric': 'euclidean', 'n_neighbors': 13, 'weights': 'distance'}\n",
      "ğŸ† Best cross-validation score: 0.9887\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ” HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Parameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  - {param}: {values}\")\n",
    "\n",
    "# Grid search\n",
    "print(\"\\nğŸ”„ Performing Grid Search with 5-fold Cross-Validation...\")\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(\n",
    "    knn, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nâœ… Grid Search completed!\")\n",
    "print(f\"\\nğŸ† Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"ğŸ† Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get best model\n",
    "best_knn = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "508c84e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ MODEL EVALUATION\n",
      "================================================================================\n",
      "\n",
      "âœ… Training Accuracy: 1.0000 (100.00%)\n",
      "âœ… Test Accuracy: 0.9920 (99.20%)\n",
      "\n",
      "ğŸ“Š Classification Report (Test Set):\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          normal     0.9951    0.9956    0.9954      4303\n",
      "severely stunted     0.9931    0.9893    0.9912      1304\n",
      "         stunted     0.9729    0.9773    0.9751       883\n",
      "          tinggi     0.9935    0.9928    0.9932      1395\n",
      "\n",
      "        accuracy                         0.9920      7885\n",
      "       macro avg     0.9887    0.9888    0.9887      7885\n",
      "    weighted avg     0.9920    0.9920    0.9920      7885\n",
      "\n",
      "\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[4284    0   10    9]\n",
      " [   0 1290   14    0]\n",
      " [  11    9  863    0]\n",
      " [  10    0    0 1385]]\n",
      "\n",
      "ğŸ“Š Detailed Confusion Matrix:\n",
      "                          Predicted: normal  Predicted: severely stunted  \\\n",
      "Actual: normal                         4284                            0   \n",
      "Actual: severely stunted                  0                         1290   \n",
      "Actual: stunted                          11                            9   \n",
      "Actual: tinggi                           10                            0   \n",
      "\n",
      "                          Predicted: stunted  Predicted: tinggi  \n",
      "Actual: normal                            10                  9  \n",
      "Actual: severely stunted                  14                  0  \n",
      "Actual: stunted                          863                  0  \n",
      "Actual: tinggi                             0               1385  \n",
      "\n",
      "ğŸ”„ Cross-Validation Scores (5-fold):\n",
      "Scores: [0.98811034 0.98937857 0.98906151 0.98858592 0.98842739]\n",
      "Mean: 0.9887\n",
      "Std: 0.0005\n",
      "95% Confidence Interval: 0.9887 (+/- 0.0009)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ˆ MODEL EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_knn.predict(X_test_scaled)\n",
    "y_pred_train = best_knn.predict(X_train_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nâœ… Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"âœ… Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nğŸ“Š Classification Report (Test Set):\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred, \n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nğŸ“Š Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Detailed confusion matrix with labels\n",
    "print(\"\\nğŸ“Š Detailed Confusion Matrix:\")\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"Actual: {cls}\" for cls in label_encoder.classes_],\n",
    "    columns=[f\"Predicted: {cls}\" for cls in label_encoder.classes_]\n",
    ")\n",
    "print(cm_df)\n",
    "\n",
    "# Cross-validation scores\n",
    "print(\"\\nğŸ”„ Cross-Validation Scores (5-fold):\")\n",
    "cv_scores = cross_val_score(best_knn, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Scores: {cv_scores}\")\n",
    "print(f\"Mean: {cv_scores.mean():.4f}\")\n",
    "print(f\"Std: {cv_scores.std():.4f}\")\n",
    "print(f\"95% Confidence Interval: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c32134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ’¾ SAVING MODEL & COMPONENTS\n",
      "================================================================================\n",
      "âœ… Model saved: C:\\Users\\muham\\Project\\Stunting\\models\\knn_model.pkl\n",
      "âœ… Scaler saved: C:\\Users\\muham\\Project\\Stunting\\models\\scaler.pkl\n",
      "âœ… Encoders saved: C:\\Users\\muham\\Project\\Stunting\\models\\encoders.pkl\n",
      "âœ… Metadata saved: C:\\Users\\muham\\Project\\Stunting\\models\\model_metadata.pkl\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL COMPONENTS SAVED SUCCESSFULLY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ’¾ SAVING MODEL & COMPONENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save KNN model\n",
    "model_path = os.path.join(MODEL_DIR, 'knn_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_knn, f)\n",
    "print(f\"âœ… Model saved: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = os.path.join(MODEL_DIR, 'scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"âœ… Scaler saved: {scaler_path}\")\n",
    "\n",
    "# Save encoders\n",
    "encoders_path = os.path.join(MODEL_DIR, 'encoders.pkl')\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'gender_encoder': gender_encoder,\n",
    "        'label_encoder': label_encoder\n",
    "    }, f)\n",
    "print(f\"âœ… Encoders saved: {encoders_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'train_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'train_accuracy': float(train_accuracy),\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'cv_mean': float(cv_scores.mean()),\n",
    "    'cv_std': float(cv_scores.std()),\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'feature_names': feature_columns,\n",
    "    'target_classes': label_encoder.classes_.tolist(),\n",
    "    'n_train_samples': int(X_train.shape[0]),\n",
    "    'n_test_samples': int(X_test.shape[0])\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(MODEL_DIR, 'model_metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"âœ… Metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ALL COMPONENTS SAVED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdb4076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ§ª TESTING MODEL PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Gender encoder classes: ['laki-laki' 'perempuan']\n",
      "\n",
      "ğŸ“ Running test predictions...\n",
      "\n",
      "Test Case 1:\n",
      "  Input: Age=12 months, Gender=laki-laki, Height=70cm\n",
      "  Prediction: stunted\n",
      "  Risk: 100.00%\n",
      "  Probabilities:\n",
      "    - normal: 0.00%\n",
      "    - severely stunted: 0.00%\n",
      "    - stunted: 100.00%\n",
      "    - tinggi: 0.00%\n",
      "\n",
      "Test Case 2:\n",
      "  Input: Age=24 months, Gender=laki-laki, Height=87cm\n",
      "  Prediction: normal\n",
      "  Risk: 0.00%\n",
      "  Probabilities:\n",
      "    - normal: 100.00%\n",
      "    - severely stunted: 0.00%\n",
      "    - stunted: 0.00%\n",
      "    - tinggi: 0.00%\n",
      "\n",
      "Test Case 3:\n",
      "  Input: Age=36 months, Gender=perempuan, Height=85cm\n",
      "  Prediction: stunted\n",
      "  Risk: 100.00%\n",
      "  Probabilities:\n",
      "    - normal: 0.00%\n",
      "    - severely stunted: 0.00%\n",
      "    - stunted: 100.00%\n",
      "    - tinggi: 0.00%\n",
      "\n",
      "Test Case 4:\n",
      "  Input: Age=48 months, Gender=laki-laki, Height=103cm\n",
      "  Prediction: normal\n",
      "  Risk: 0.00%\n",
      "  Probabilities:\n",
      "    - normal: 100.00%\n",
      "    - severely stunted: 0.00%\n",
      "    - stunted: 0.00%\n",
      "    - tinggi: 0.00%\n",
      "\n",
      "Test Case 5:\n",
      "  Input: Age=6 months, Gender=perempuan, Height=60cm\n",
      "  Prediction: stunted\n",
      "  Risk: 100.00%\n",
      "  Probabilities:\n",
      "    - normal: 0.00%\n",
      "    - severely stunted: 0.00%\n",
      "    - stunted: 100.00%\n",
      "    - tinggi: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ§ª TESTING MODEL PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# First, check what gender values are in the dataset\n",
    "print(\"\\nğŸ“‹ Gender encoder classes:\", gender_encoder.classes_)\n",
    "\n",
    "# Test cases - updated to match actual dataset gender values\n",
    "test_cases = [\n",
    "    {'age': 12, 'gender': gender_encoder.classes_[0], 'height': 70, 'expected': 'Possibly stunted'},\n",
    "    {'age': 24, 'gender': gender_encoder.classes_[0], 'height': 87, 'expected': 'Normal'},\n",
    "    {'age': 36, 'gender': gender_encoder.classes_[1], 'height': 85, 'expected': 'Possibly stunted'},\n",
    "    {'age': 48, 'gender': gender_encoder.classes_[0], 'height': 103, 'expected': 'Normal'},\n",
    "    {'age': 6, 'gender': gender_encoder.classes_[1], 'height': 60, 'expected': 'Possibly stunted'}\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“ Running test predictions...\\n\")\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    # Encode gender\n",
    "    gender_encoded = gender_encoder.transform([case['gender']])[0]\n",
    "    \n",
    "    # Prepare input\n",
    "    X_test_case = np.array([[case['age'], gender_encoded, case['height']]])\n",
    "    X_test_case_scaled = scaler.transform(X_test_case)\n",
    "    \n",
    "    # Predict\n",
    "    pred_encoded = best_knn.predict(X_test_case_scaled)[0]\n",
    "    prediction = label_encoder.inverse_transform([pred_encoded])[0]\n",
    "    \n",
    "    # Get probabilities\n",
    "    probabilities = best_knn.predict_proba(X_test_case_scaled)[0]\n",
    "    prob_dict = dict(zip(label_encoder.classes_, probabilities))\n",
    "    \n",
    "    # Calculate stunting risk\n",
    "    risk_classes = ['severely stunted', 'stunted']\n",
    "    risk_percentage = sum([prob_dict.get(cls, 0) for cls in risk_classes]) * 100\n",
    "    \n",
    "    print(f\"Test Case {i}:\")\n",
    "    print(f\"  Input: Age={case['age']} months, Gender={case['gender']}, Height={case['height']}cm\")\n",
    "    print(f\"  Prediction: {prediction}\")\n",
    "    print(f\"  Risk: {risk_percentage:.2f}%\")\n",
    "    print(f\"  Probabilities:\")\n",
    "    for status, prob in prob_dict.items():\n",
    "        print(f\"    - {status}: {prob*100:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e11b1ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“‹ MODEL TRAINING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "âœ… Model Training Completed Successfully!\n",
      "\n",
      "ğŸ“Š Dataset Information:\n",
      "   - Total samples: 39425\n",
      "   - Training samples: 31540\n",
      "   - Test samples: 7885\n",
      "   - Features: 3\n",
      "   - Classes: 4\n",
      "\n",
      "ğŸ¯ Model Performance:\n",
      "   - Training Accuracy: 100.00%\n",
      "   - Test Accuracy: 99.20%\n",
      "   - CV Mean Score: 98.87%\n",
      "   - CV Std: 0.05%\n",
      "\n",
      "ğŸ† Best Hyperparameters:\n",
      "   - n_neighbors: 13\n",
      "   - weights: distance\n",
      "   - metric: euclidean\n",
      "\n",
      "ğŸ’¾ Saved Files:\n",
      "   - knn_model.pkl\n",
      "   - scaler.pkl\n",
      "   - encoders.pkl\n",
      "   - model_metadata.pkl\n",
      "\n",
      "ğŸ“ Location: C:\\Users\\muham\\Project\\Stunting\\models\n",
      "\n",
      "ğŸš€ Next Steps:\n",
      "   1. Model sudah siap digunakan\n",
      "   2. Jalankan Streamlit app dengan command:\n",
      "      streamlit run app.py\n",
      "   3. Buka browser di localhost:8501\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“‹ MODEL TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "âœ… Model Training Completed Successfully!\n",
    "\n",
    "ğŸ“Š Dataset Information:\n",
    "   - Total samples: {len(df_clean)}\n",
    "   - Training samples: {len(X_train)}\n",
    "   - Test samples: {len(X_test)}\n",
    "   - Features: {len(feature_columns)}\n",
    "   - Classes: {len(label_encoder.classes_)}\n",
    "\n",
    "ğŸ¯ Model Performance:\n",
    "   - Training Accuracy: {train_accuracy*100:.2f}%\n",
    "   - Test Accuracy: {test_accuracy*100:.2f}%\n",
    "   - CV Mean Score: {cv_scores.mean()*100:.2f}%\n",
    "   - CV Std: {cv_scores.std()*100:.2f}%\n",
    "\n",
    "ğŸ† Best Hyperparameters:\n",
    "   - n_neighbors: {grid_search.best_params_['n_neighbors']}\n",
    "   - weights: {grid_search.best_params_['weights']}\n",
    "   - metric: {grid_search.best_params_['metric']}\n",
    "\n",
    "ğŸ’¾ Saved Files:\n",
    "   - {os.path.basename(model_path)}\n",
    "   - {os.path.basename(scaler_path)}\n",
    "   - {os.path.basename(encoders_path)}\n",
    "   - {os.path.basename(metadata_path)}\n",
    "\n",
    "ğŸ“ Location: {MODEL_DIR}\n",
    "\n",
    "ğŸš€ Next Steps:\n",
    "   1. Model sudah siap digunakan\n",
    "   2. Jalankan Streamlit app dengan command:\n",
    "      streamlit run app.py\n",
    "   3. Buka browser di localhost:8501\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82321082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Launching Streamlit App...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸš€ Launching Streamlit App...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Pastikan file app.py ada di directory yang sama\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Get the directory of this notebook\n",
    "notebook_dir = BASE_PATH\n",
    "\n",
    "print(\"\\nğŸ“ To run the Streamlit app manually, execute in terminal:\")\n",
    "print(f\"   cd {BASE_PATH}\")\n",
    "print(f\"   {sys.executable} -m streamlit run app.py\")\n",
    "print(\"\\nOr simply:\")\n",
    "print(\"   python -m streamlit run app.py\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… NOTEBOOK EXECUTION COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ’¡ Note: If 'streamlit' command not found, use:\")\n",
    "print(\"   python -m streamlit run app.py\")\n",
    "print(\"\\nğŸŒ The app will open in your browser at: http://localhost:8501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970ce9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
